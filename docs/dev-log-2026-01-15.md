# Rescue Rover - Dev Log: January 15, 2026

## Session Summary

This session focused on debugging the **Hybrid Cloud AI system** and **UI integration** for the Rescue Rover project.

---

## üéØ Goals

1. Restore the original React-based UI (Figma design)
2. Fix VLM (Vision Language Model) connection errors
3. Verify the hybrid AI pipeline (YOLO local + VLM cloud)

---

## üîß Issues Encountered & Fixes

### 1. Legacy UI Not Updating (Placeholders)

**Problem:** The original React frontend (`frontend/index.html`) displayed only placeholders - no live data.

**Investigation:**
- The frontend is pre-compiled (minified JS), making it impossible to see exact API endpoints
- Grepped for URLs in `index-*.js` - found `http://` references and `8080` port, but no API paths
- The old `RoverInterface-OLD/app.py` showed expected endpoints: `/api/telemetry`, `/api/mission_log`

**Attempted Fixes:**
- Reverted `app.py` to serve frontend via iframe
- Added video streaming endpoints (`/stream`, `/video_feed`)
- Mapped telemetry keys (`voltage` ‚Üí `battery`)

**Status:** ‚è≥ Waiting for frontend source code from teammate to properly debug

---

### 2. VLM "Server Error" Messages

**Problem:** Console logs showed:
```
üß† VLM: center - Server Error
```

**Root Cause Analysis:**

| Error Message | Cause |
|---------------|-------|
| `Server Error` | Generic catch-all from Colab exception handler |
| `Failed to apply prompt replacement for m` | Wrong image placeholder format (`<image>`) |
| `Unknown part type: image` | vLLM API doesn't support `{"type": "image"}` in chat messages |
| `EngineCore encountered an issue` | vLLM internal error, possibly OOM or config issue |

**Solution Found (via Perplexity/Firecrawl research):**

The correct Qwen2.5-VL prompt format for vLLM requires:
```python
prompt = (
    "<|im_start|>system\n...<|im_end|>\n"
    "<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>"
    f"{question}<|im_end|>\n"
    "<|im_start|>assistant\n"
)

outputs = llm.generate({
    "prompt": prompt,
    "multi_modal_data": {"image": pil_image},
}, sampling_params=sampling_params)
```

**Key insight:** Use `<|vision_start|><|image_pad|><|vision_end|>` instead of `<image>`.

---

### 3. Colab Server Crashes

**Problem:** Server would work for a while then shut down.

**Evidence from logs:**
```
INFO: Shutting down
INFO: Finished server process [1198]
```

**Cause:** Colab runtime timeout or cell re-execution issue.

**Fix:** Re-run the server cell when it stops. Consider using `while True` loop with restart logic.

---

## üìÅ Files Modified

| File | Changes |
|------|---------|
| `RoverInterface/app.py` | Added `/stream`, `/video_feed` endpoints; mapped `voltage`‚Üí`battery` |
| `RoverInterface/ai/strategic_navigator.py` | Added `import io` (fixed earlier session) |
| `RoverInterface/ai/colab_server_script.py` | Improved error logging |
| `RoverInterface/test_ai.py` | Created standalone test interface |

---

## üß™ Test Interface Created

Created `test_ai.py` - a standalone script to verify the AI pipeline:
- Opens webcam
- Runs YOLO every 3 frames (for performance)
- Calls VLM every 2 seconds (async)
- Displays HUD overlay with decisions
- Prints debug info to console

Usage:
```bash
cd RoverInterface
python test_ai.py
```

---

## üìã Correct Colab Cell [3] Code

```python
app = FastAPI()

@app.post("/analyze")
async def analyze_image(file: UploadFile = File(...)):
    try:
        content = await file.read()
        image = Image.open(io.BytesIO(content)).convert("RGB")
        
        question = """Output JSON ONLY: {"hazard": boolean, "steering": "left"|"right"|"center"|"stop", "reasoning": "short explanation"}
Where should I drive?"""

        prompt = (
            "<|im_start|>system\nYou are a robot navigator.<|im_end|>\n"
            "<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>"
            f"{question}<|im_end|>\n"
            "<|im_start|>assistant\n"
        )
        
        sampling_params = SamplingParams(temperature=0.1, max_tokens=128)
        
        outputs = llm.generate({
            "prompt": prompt,
            "multi_modal_data": {"image": image},
        }, sampling_params=sampling_params)
        
        generated_text = outputs[0].outputs[0].text
        return {"result": generated_text}
        
    except Exception as e:
        import traceback
        print(f"‚ùå ANALYZE ERROR: {e}")
        traceback.print_exc()
        return {"error": str(e), "result": json.dumps({"hazard": True, "steering": "stop", "reasoning": f"ERR: {str(e)[:40]}"})}
```

---

## ‚úÖ EngineCore Fix (RESOLVED)

**Problem:** `EngineDeadError: EngineCore encountered an issue`

**Root Cause:** vLLM v1 engine instability + high VRAM usage from KV-cache

**Solution Applied:**

1. **Disable vLLM v1 engine** (add before importing vLLM):
```python
import os
os.environ["VLLM_USE_V1"] = "0"
from vllm import LLM
```

2. **Reduce KV-cache pressure**:
```python
llm = LLM(
    model=MODEL_ID,
    dtype="float16",
    gpu_memory_utilization=0.80,  # Reduced from 0.90
    max_model_len=4096,           # Reduced from 8192
    limit_mm_per_prompt={"image": 1},
)
```

**Result:** ‚úÖ VLM now responds correctly:
```
[DEBUG VLM RAW]: {"hazard": false, "steering": "center", "reasoning": "..."}
```

**Performance:** ~1s per inference, ~2986 toks/s input, ~52 toks/s output

---

## ‚è≠Ô∏è Next Steps

1. **Get frontend source code** from teammate to fix UI data binding
2. **Alternative:** Build native NiceGUI dashboard if frontend source unavailable
3. **Integration:** Connect working VLM pipeline to main `app.py`

---

## üîó Useful References

- [vLLM Multimodal Inputs Docs](https://docs.vllm.ai/en/stable/features/multimodal_inputs.html)
- [Qwen2.5-VL vLLM Example](https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/vision_language.py)
- [Qwen Deployment Guide](https://qwen.readthedocs.io/en/latest/deployment/vllm.html)

---

## üìä Session Stats

- **Duration:** ~1 hour
- **Main blockers:** vLLM API format changes, opaque frontend code
- **Tools used:** Perplexity search, Firecrawl scraping, grep, Python debugging
