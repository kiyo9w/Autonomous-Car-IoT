{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "print(\"üöÄ Installing dependencies (vLLM, ngrok, fastapi)...\")\n",
        "subprocess.run(\"pip install -q vllm pyngrok uvicorn fastapi python-multipart nest-asyncio pillow\", shell=True)"
      ],
      "metadata": {
        "id": "pQRJ1zZttFso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from vllm import LLM, SamplingParams\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "\n",
        "# Use 7B model for A100. If using T4 (Free), switch to \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "MODEL_ID = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
        "# MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\" # Uncomment for Free T4 GPU\n",
        "\n",
        "# Ngrok Auth Token (Set from user input)\n",
        "ngrok.set_auth_token(\"38HpmQZrcDwmGKHwpgo8rFljVGY_7kgt4BNgpzhWRn8RAfjR1\")\n",
        "\n",
        "# --- 4. Initialize Model (vLLM) ---\n",
        "print(f\"üîÑ Loading Model: {MODEL_ID}...\")\n",
        "try:\n",
        "    # UPDATED FIX: Limit context to 8k and optimize memory\n",
        "    llm = LLM(\n",
        "        model=MODEL_ID, \n",
        "        dtype=\"float16\", \n",
        "        gpu_memory_utilization=0.90, \n",
        "        trust_remote_code=True,\n",
        "        max_model_len=8192,\n",
        "        limit_mm_per_prompt={\"image\": 1}\n",
        "    )\n",
        "    print(\"‚úÖ Model Loaded Successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model Load Error: {e}\")\n",
        "    print(\"If OOM on T4, try the 3B model or set gpu_memory_utilization=0.8\")\n",
        "    raise e"
      ],
      "metadata": {
        "id": "tMqpoc_gtNM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "async def analyze_image(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        # Read uploaded image\n",
        "        content = await file.read()\n",
        "        image = Image.open(io.BytesIO(content)).convert(\"RGB\")\n",
        "\n",
        "        # Prepare Prompt (Optimized for Navigation)\n",
        "        system_prompt = \"\"\"You are a robot navigator. Analyze the scene for walkability.\n",
        "Output JSON ONLY:\n",
        "{\n",
        "    \"hazard\": boolean,\n",
        "    \"nav_goal\": \"open_space\" | \"follow_path\" | \"avoid_obstacle\",\n",
        "    \"steering\": \"left\" | \"right\" | \"center\" | \"stop\",\n",
        "    \"reasoning\": \"short explanation\"\n",
        "}\"\"\"\n",
        "\n",
        "        user_prompt = \"Analyze this view. Where should I drive?\"\n",
        "\n",
        "        # Generate using vLLM\n",
        "        inputs = {\n",
        "            \"prompt\": f\"{system_prompt}\\nUser: <image>\\n{user_prompt}\\nAssistant:\",\n",
        "            \"multi_modal_data\": {\"image\": image},\n",
        "        }\n",
        "\n",
        "        sampling_params = SamplingParams(temperature=0.1, max_tokens=128)\n",
        "\n",
        "        outputs = llm.generate(inputs, sampling_params=sampling_params)\n",
        "        generated_text = outputs[0].outputs[0].text\n",
        "\n",
        "        return {\"result\": generated_text}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e), \"result\": json.dumps({\"hazard\": True, \"reasoning\": \"Server Error\"})}"
      ],
      "metadata": {
        "id": "eQDa45PJtSZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Tunnel\n",
        "port = 8000\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\"\\nExample config.py setting:\\nREMOTE_VLM_URL = \\\"{public_url}/analyze\\\"\")\n",
        "print(f\"üöÄ API Live at: {public_url}\")\n",
        "\n",
        "# Run FastAPI\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=port)\n"
      ],
      "metadata": {
        "id": "U7Q0AKAPs2Kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}